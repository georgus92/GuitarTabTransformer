# Transformer Tablature: Deep Learning for Guitar Transcription

This repository contains all Python modules, training scripts, and utilities needed to reproduce the experiments described in the dissertation. 

---

## ğŸ“ Project summary

The goal is to translate polyphonic guitar audio into humanâ€‘readable tablature with expressive technique annotations.  
Key ingredients:

* **Dataset** â€“ [SynthTab](https://github.com/yongyizang/SynthTab) (subset used for storage constraints)  
* **Input** â€“ Constantâ€‘Q Transform (CQT) spectrograms (84 bins, 512â€‘hop, 16â€¯kHz)  
* **Model** â€“ Custom Transformer encoderâ€‘decoder (â‰ˆâ€¯9â€¯M params) implemented in PyTorch  
* **Output tokens** â€“ `(string, fret, duration, technique)` tuples covering bends, slides, hammerâ€‘ons, etc.  

---

## ğŸ“ Repository structure

```
.
â”œâ”€â”€ src/                         
â”‚   â”œâ”€â”€ guitar_tab_dataset_loader.py
â”‚   â”œâ”€â”€ precomputed_dataset_loader.py
â”‚   â”œâ”€â”€ guitar_tab_collate.py
â”‚   â”œâ”€â”€ split_dataset.py
â”‚   â”œâ”€â”€ tab_transformer_model.py
â”‚   â”œâ”€â”€ tab_transformer_training.py
â”‚   â”œâ”€â”€ generate_and_precompute.py   # feature extraction + token generation
â”‚   â”œâ”€â”€ train_transformer.py         
â”‚   â”œâ”€â”€ evaluate.py                  # load checkpoint & render TAB
â”‚   â”œâ”€â”€ guitar_tab_vocab_with_extended_techniques.json
â”‚   â””â”€â”€ note_tab.json               # custom JAMS namespace schema
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```
---

## ğŸš€ Quick start

Make sure that the paths in the code are correct.

### 1Â Â Set up environment

```bash
git clone https://github.com/georgus92/GuitarTabTransformer.git
cd GuitarTabTransformer

python -m venv .venv        # or conda create â€‘n gtt python=3.10
source .venv/bin/activate   # Windows: .venv\Scripts\activate
pip install -r requirements.txt
```

### 2Â Â Generate CQT + tokens

```bash
python scripts/generate_and_precompute.py
```

This writes `*.pt` tensors (CQT) and token files into `precomputed/`.

### 3Â Â Split dataset

```bash
python scripts/split_dataset.py
```

### 4Â Â Train

```bash
python train_transformer_amp.py --batch_size 4 --epochs 40 --lr 1e-4
```

### 5Â Â Evaluate and render TAB

```bash
python scripts/evaluate.py
```

The script prints a 6â€‘line ASCII TAB and saves a `.txt` file next to the audio.

---

## ğŸ›  Requirements

* Python â‰¥â€¯3.10  
* PyTorchÂ â‰¥â€¯2.2 + torchaudio  
* librosa, JAMS, NumPy, Matplotlib, Seaborn  
* (Optional) CUDAâ€‘enabled GPU for training

---

## ğŸ”¬ Reproducibility checklist

1. Download the full SynthTab dataset.  
2. Run `generate_and_precompute.py` to build the precomputed chunk cache.  
3. Train with the hyperâ€‘parameters batch_size=4 epochs=40 lr=1e-4.  
4. Evaluate on the heldâ€‘out test split.  

Every step is scripted; no notebook magic required.

